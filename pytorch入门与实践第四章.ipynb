{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.8215,  2.2869, -3.2666],\n",
      "        [-1.5071,  0.5723,  0.0329]])\n"
     ]
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_features,out_features):\n",
    "        super(Linear,self).__init__()\n",
    "        self.w  = nn.Parameter(t.randn(in_features,out_features))\n",
    "        self.b = nn.Parameter(t.randn(out_features))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.mm(self.w)\n",
    "        return x + self.b\n",
    "\n",
    "layer = Linear(4,3)\n",
    "input = V(t.randn(2,4))\n",
    "output = layer(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[ 1.1823, -0.8361, -1.2115],\n",
      "        [ 0.4752, -0.0507, -0.6972],\n",
      "        [ 0.6956, -0.2602,  0.7799],\n",
      "        [-1.0541,  0.2979, -1.7820]])\n",
      "b Parameter containing:\n",
      "tensor([-0.6705,  0.3712, -2.8513])\n"
     ]
    }
   ],
   "source": [
    "for name,parameter in layer.named_parameters():\n",
    "    print(name,parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.w torch.Size([3, 4])\n",
      "layer1.b torch.Size([4])\n",
      "layer2.w torch.Size([4, 1])\n",
      "layer2.b torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self,in_features,hidden_features,out_features):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer1 = Linear(in_features,hidden_features)\n",
    "        self.layer2 = Linear(hidden_features,out_features)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = layer1(x)\n",
    "        x = nn.Sigmoid(x)\n",
    "        out = layer2(x)\n",
    "        return out\n",
    "\n",
    "perceptron = Perceptron(3,4,1)\n",
    "for name,parameter in perceptron.named_parameters():\n",
    "    print(name,parameter.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage,ToTensor\n",
    "to_tensor = ToTensor()\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "lena = Image.open('F:/Pycharm/lena.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight[1, 1, 3, 3], so expected input[1, 3, 300, 300] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a80a0111a044>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mto_pil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight[1, 1, 3, 3], so expected input[1, 3, 300, 300] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "input1 = to_tensor(lena).unsqueeze(0)\n",
    "kernel = t.ones(3,3) / -9\n",
    "kernel[1][1] = 1\n",
    "conv = nn.Conv2d(1,1,(3,3),1,bias=False)\n",
    "conv.weight.data = kernel.view(1,1,3,3)\n",
    "out = conv(V(input1))\n",
    "to_pil(out.data.squeeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2236,  1.0610,  0.2820, -0.1842],\n",
      "        [-0.0414, -1.0146,  0.1228, -0.3233]])\n"
     ]
    }
   ],
   "source": [
    "input1 = V(t.randn(2,3))\n",
    "linear = nn.Linear(3,4)\n",
    "h = linear(input1)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.00000e-07 *\n",
       "        [ 0.0000,  0.0000,  0.0000, -4.7684]),\n",
       " tensor([ 31.9615,  31.9997,  31.9496,  31.9340]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(4)\n",
    "bn.weight.data = t.ones(4) * 4\n",
    "bn.bias.data = t.zeros(4)\n",
    "bn_out = bn(h)\n",
    "bn_out.mean(0),bn_out.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4587, -1.1515,  0.3176],\n",
      "        [-0.7854,  0.4379,  0.7150]])\n",
      "tensor([[ 0.0000,  0.0000,  0.3176],\n",
      "        [ 0.0000,  0.4379,  0.7150]])\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU(inplace=True)\n",
    "input = V(t.randn(2,3))\n",
    "\n",
    "print(input)\n",
    "output = relu(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = nn.Sequential()\n",
    "net1.add_module('conv',nn.Conv2d(3,3,3))\n",
    "net1.add_module('batchnorm',nn.BatchNorm2d(3))\n",
    "net1.add_module('activation_layer',nn.ReLU())\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "nn.Conv2d(3,3,3),\n",
    "nn.BatchNorm2d(3),\n",
    "nn.ReLU()\n",
    ")\n",
    "from collections import OrderedDict\n",
    "\n",
    "net3 = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mymodule(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Mymodule(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.list = [nn.Linear(3,4),nn.ReLU()]\n",
    "        self.module_list = nn.ModuleList([nn.Conv2d(3,3,3),nn.ReLU()])\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "module = Mymodule()\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_list.0.weight Parameter containing:\n",
      "tensor([[[[-0.1568, -0.1539,  0.1271],\n",
      "          [-0.0341,  0.1552, -0.0705],\n",
      "          [-0.0228, -0.0531,  0.1033]],\n",
      "\n",
      "         [[-0.0290, -0.0612,  0.1752],\n",
      "          [-0.0443,  0.1360,  0.0650],\n",
      "          [-0.1013, -0.1112, -0.0380]],\n",
      "\n",
      "         [[ 0.0432, -0.0138,  0.0361],\n",
      "          [-0.0035, -0.0582, -0.0646],\n",
      "          [ 0.1375, -0.1269,  0.0359]]],\n",
      "\n",
      "\n",
      "        [[[-0.0078,  0.1323, -0.0875],\n",
      "          [-0.0077,  0.0815,  0.0843],\n",
      "          [-0.1312,  0.1130, -0.0558]],\n",
      "\n",
      "         [[ 0.1307,  0.1370, -0.0895],\n",
      "          [ 0.0886, -0.1650,  0.1579],\n",
      "          [ 0.0744, -0.0384,  0.0151]],\n",
      "\n",
      "         [[-0.0776, -0.0704,  0.0692],\n",
      "          [ 0.0467, -0.0395,  0.1517],\n",
      "          [ 0.1763,  0.0974, -0.0783]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1426,  0.1073,  0.1921],\n",
      "          [ 0.0063,  0.0546, -0.0974],\n",
      "          [ 0.0410,  0.0155, -0.0944]],\n",
      "\n",
      "         [[-0.1633,  0.0394,  0.0057],\n",
      "          [-0.1708, -0.1686, -0.1061],\n",
      "          [-0.1124, -0.1902,  0.0418]],\n",
      "\n",
      "         [[ 0.0101,  0.0668,  0.0052],\n",
      "          [-0.1148, -0.0038,  0.0935],\n",
      "          [-0.1388,  0.1554, -0.0121]]]])\n",
      "module_list.0.bias Parameter containing:\n",
      "tensor([-0.1791, -0.0787, -0.0073])\n"
     ]
    }
   ],
   "source": [
    "for name,param in module.named_parameters():\n",
    "    print(name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669],\n",
      "        [ 0.0617,  0.6213],\n",
      "        [-0.4519, -0.1661]])\n",
      "tensor(0.8272)\n"
     ]
    }
   ],
   "source": [
    "t.manual_seed(1)\n",
    "score = V(t.randn(3,2))\n",
    "print(score)\n",
    "label = V(t.Tensor([1,0,1])).long()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(score,label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化器\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.features = nn.Sequential(\n",
    "                    nn.Conv2d(3,6,5),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2,2),\n",
    "                    nn.Conv2d(6,16,5),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2,2)\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                        nn.Linear(16*5*5,120),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(120,84),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(84,10)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1,16*5*5)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(params=net.parameters(),lr=1)\n",
    "optimizer.zero_grad()\n",
    "input = V(t.randn(1,3,32,32))\n",
    "output = net(input)\n",
    "\n",
    "output.backward(output)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为不同的子网络设置不同的学习率\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params':net.features.parameters()},\n",
    "    {'params':net.classifier.parameters(),'lr':1e-2}\n",
    "],lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只为两个全连接层设置较大的学习率，其余层的学习率较小\n",
    "import torch\n",
    "special_layers = nn.ModuleList([net.classifier[0],net.classifier[2]])\n",
    "special_layers_params = list(map(id,special_layers.parameters()))\n",
    "\n",
    "base_params = filter(lambda p:id(p) not in special_layers_params,net.parameters())\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params':base_params},{'params':special_layers.parameters(),'lr':0.01}],lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8794, -0.6148, -0.5532,  0.7073],\n",
      "        [-0.2332,  0.1333, -0.3331,  0.4284]])\n",
      "tensor([[-0.8794, -0.6148, -0.5532,  0.7073],\n",
      "        [-0.2332,  0.1333, -0.3331,  0.4284]])\n"
     ]
    }
   ],
   "source": [
    "input = V(t.randn(2,3))\n",
    "\n",
    "model = nn.Linear(3,4)\n",
    "\n",
    "output1 = model(input)\n",
    "output2 = nn.functional.linear(input,model.weight,model.bias)\n",
    "\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.ReLU()(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1371,  0.5824,  0.0000],\n",
      "        [ 0.3988,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.pool(F.relu(self.conv1(x)),2)\n",
    "        x = F.pool(F.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1,16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (submodule): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.params = nn.Parameter(t.randn(3,3))\n",
    "        self.submodule = nn.Linear(3,4)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        x  = self.params@input\n",
    "        x = self.submodule(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('submodule', Linear(in_features=3, out_features=4, bias=True))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('params', Parameter containing:\n",
       "              tensor([[-1.1465, -0.2239, -0.1879],\n",
       "                      [-0.2030,  1.2752,  0.1303],\n",
       "                      [ 1.6539,  0.0022, -0.1065]]))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params torch.Size([3, 3])\n",
      "submodule.weight torch.Size([4, 3])\n",
      "submodule.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
